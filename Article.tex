%#BIBTEX /Library/TeX/texbin/bibtex Article
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption} 
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\fk}[1]{\textcolor{red}{[FK:#1]}}

\begin{document}

\title{\textbf{Task learning through teleoperation\fk{too general title}\\}
}

\author{\IEEEauthorblockN{Victor Barberteguy$^{1,2}$}
\IEEEauthorblockA{}
\and
\IEEEauthorblockN{Fumio Kanehiro$^{2}$}
\IEEEauthorblockA{}
\thanks{$^{1}$Ecole Polytechnique, Palaiseau, France}
\thanks{$^{2}$CNRS-AIST JRL (Joint Robotics Laboratory), IRL, Japan}
}

\maketitle

\begin{abstract}
We provide a complete framework for learning, reproducing and generalizing tasks from human demonstrations. This method adapts recent developments in automatic, unsupervised segmentation of time-series to humanoid robotics by preprocessing the data obtained from a broad range of the robot's sensors.

In more detail,  we reproduce and extend the acquired multi-step task using Dynamic Movement Primitives in simulation for the JVRC1 Robot, and further validate it in the real world with the Kaleido Robot, thus showcasing the capacity of our approach to create an extensive library of reusable skills for complex humanoids.
\end{abstract}

\begin{IEEEkeywords}
learning from demonstration, generalization, teleoperation, probabilistic inference, feature selection, dynamic movement primitives, sensory skill integration
\end{IEEEkeywords}

\section{Introduction}
\subsection{Learning from Demonstration}

Demonstrations provide an intuitive way for non-professional users to specify tasks to the robot \cite{schaal_is_1999}. Most of robot's controllers to date, albeit successful to achieve complex tasks, often require theoretical knowledge and is time-consuming. To address these issues, research about robot learning from demonstration (LfD) has become increasingly important over the past two decades with the surge of Machine Learning (ML) algorithms \cite{argall_survey_2009} \cite{ravichandar_recent_2020}. In this framework, users can demonstrate a task that is then learned and generalized by the robot. Generalization is defined as the capacity to perform a learned task in a similar environment as the one of the demonstration, but under different initial or goal poses. LfD has been widely used and proved successful on robotic arms or in simulation \cite{pastor_learning_2009}, but are more seldom used on real humanoid robots \cite{calinon_learning_2007}due to their high number of degrees of freedom (DoF) and unlinear behavior. A core ideaof this work is to capitalize on the physiologic similarities between us, humans, and humanois robots to come up with an intuitive LfD technique.  \fk{need to confirm that something humanoid specific method is proposed}

\textbf{Note:} There is little difference between LfD and imitation learning, mainly imitation learning requires embodiment mapping \cite{argall_survey_2009}, that is to say a treatment of the data to transform it from high-level information (i.e, a video of the executed task) to low-level data (joint positions, torques) easily understandable for the robot. LfD provides directly this low-level data to the robot. In this article, we will employ both terms indifferently, as our main focus is how to deal with the low-level data in input of the robot. \newline\fk{this note is not necessary}

Several techniques are found in the literature to provide demonstrations to the robot. This can be done through simulation \cite{sammut_learning_1992}, kinesthesic teaching \cite{wu_prim-lafd:_2022}, motion capture \cite{ramirez-amaro_understanding_2015} or with teleoperation devices. These teleoperations devices appear under various shapes, from joysticks \cite{ando_master-slave_2020} to exoskeletons where the user remotely controls the robot \cite{fang_skill_2019}. In all the above cited works, some prefer 1-shot imitation learning, where the demonstration can be a seed for an initial policy that is then derived and learned through Reinforcement Learning (RL) \cite{vecerik_practical_2019} \cite{stulp_reinforcement_2012}, Inverse RL \cite{rouot_inverse_2017} or Hierarchical RL \cite{zhao_variational_2023} that can possibly be corrected with online coaching \cite{advice_operator}. Another recent approach is to create more demonstrations with the initial one, thus enriching the dataset thanks to meta-learning processes \cite{yu_one-shot_2018}. On the contrary, other methods rely on a set of demonstrations to perform probabilistic inference based on Hidden Markov Models \cite{rana_towards_2017}, Neural Networks \cite{zhang_deep_2018} or using Granger Causality \cite{chuck2023grangercausal}. The former often derives only one policy that is locally optima, and thus the learning process is repeated for each task, whereas the latter methods prefer to decompose the task in elementary \textit{skills} with segmentation methods, thus enhancing the reusability of the stored skills. Our proposed method is of the latter category, that is further detailed in \ref{Approach}

Whatever the method used, once the task learned, the great majority of the references cited above use Dynamic Movement Primitives (DMP), that were first introduced by Ijspeert and Schaal in \cite{ijspeert_movement_2002} and futher explored in \cite{ijspeert_dynamical_2013}. It provides a suitable framework to reproduce trajectories and to smoothly adapt to new conditions.

\subsection{Related work} \label{Approach}

Splitting a complex task such as grasping an object or pouring water has the advantage of making the result more generalizable. Indeed, every part of the trajectory (every \textit{skill}) has its own level of generalizability thanks to DMP, thus resulting in an intra-task generalization, better than a single end-to-end DMP. Nonetheless, a complex task is almost all the time demonstrated without specifying the number of segments: the demonstrator naturally achieves the task. Consequently, one challenge of this approach is to automatically segment the trajectory in human-interpretable skills that, once stored in a library, can be reused to learn new skills \cite{meier_movement_2011}.

Recent methods provide plans to the robot about the different phases of the demonstration using natural language or computer vision \cite{caccavale_kinesthetic_2019} \cite{saran_enhancing_2019}. Besides, probabilistic approaches are employed to segment a demonstration in a unsupervised fashion. The number of phases involved as well as the temporal position of changepoint (CP) are inferred automatically. These techniques include Hidden Markov Models (HMM) \cite{niekum_learning_2015}, Gaussian Mixture Regression (GMR) \cite{calinon_learning_2010} \cite{calinon_learning_2007} and more seldom reinforcement learning techniques \cite{kroemer_towards_2015}. Throughout the upcited references, the features that are selected to learn the task are manually selected by the user. Working with such specially chosen features often produces satisfactory results inasmuch as the skilled demonstrator knows which feature (positions, orientations, state of the gripper, ...) is relevant to study. However, the feature selection has to be rethinked from scratch for each new task.\newline

\fk{The objective of this research must be state clearly earlier.}

\textit{Contributions:} 

Our method addresses all the upcited issues in a comprehensive framework that combines state-of-the-art LfD techniques with unsupervised automatic segmentation to achieve robust task learning and generalization. We primarily fetch almost all the data available, whether it is kinetic  or sensory, from a small number of demonstrations and automatically selects the most relevant features to study with Dynamic Time Warping (DTW) \fk{reference}and correlation analysis. Segmentation is achieved with Bayesian Online Changepoint Detection with Online Detection (BOCPDMS) (or the one we will choose) and task reproduction and generalization is performed with DMP \cite{ijspeert_movement_2002} \cite{ijspeert_dynamical_2013}. This framework is applied in simulation on the JVRC1 and in real world on the Kaleido humanoid robot.\fk{clarify differences with related works.}

\subsection{Outline}

Section \ref{Background} details the theoretical aspects of the different steps of the proposed framework. Following that, section \ref{LfD} delves into how the previously described concepts are applied to our learning and generalizing a task to humanoid robots. Section \ref{results} puts on display the results obtained and then compared to other existing methods. Limits and future work possibilities are finally explored in section \ref{discussion}.


\section{Background} \label{Background}

\subsection{Bayesian Changepoint detection}

Theory on Bayesian Online Changepoint Detection

\subsection{Dynamic Movement Primitives}

The concept of Discrete DMP was first introduced by Ispjeert and al. in 2002 \cite{ijspeert_movement_2002}, and more accurately developed in 2013 \cite{ijspeert_dynamical_2013}. The discrete DMP is utilized to encode a point-to-point motion into a stable dynamical system (e.g, a damped oscillator).

 A DMP for 1-DoF trajectory $y$ of a discrete movement (point-to-point) is defined by the following set of nonlinear differential equations \cite{ijspeert_movement_2002}:
\begin{align}
\tau \dot{z} &= \alpha_z \left(\beta_z(g - y) - z\right) + f(x),  \\
\tau \dot{y} &= z, \\
\tau \dot{x} &= \alpha_x x, 
\end{align}

where $x$ is the phase variable and $z$ is an auxiliary variable. Parameters $\alpha_z$ and $\beta_z$ define the behavior of the second-order system described by Equations (1) \fk{use ref command to keep consistency}and (2). Ensuring conditions $\tau > 0$, $\alpha_z = 4\beta_z$, and $\alpha_x > 0$ leads the dynamical system  to converge towards a unique attractor point at $y=g$, $z=0$ \cite{ijspeert_dynamical_2013}. Alternatively, the gains $\alpha_z$ and $\beta_z$ can be learned from training data while preserving the convergence of the system (Tan et al. 2016\fk{reference}). In the DMP literature, Equations (1)-(2) are often referred to as the transformation system, while Equation (3) represents the canonical system. $f(x)$ is defined as a linear combination of $N$ nonlinear Radial Basis Functions (RBFs), which allows the robot to follow any smooth trajectory from  $y_0$ to the goal $g$.

\begin{align}
f(x) &= \sum_{i=1}^{N} P_i \Psi_i(x) x,  \\
\Psi_i(x) &= \exp \left(-h_i (x - c_i)^2\right),  
\end{align}

In Equation (4),  $P_i$ represents the weight associated with the $i$-th term, $\Psi_i(x)$ is defined as the $i$-th Radial Basis Function (RBF), and $x$ is the phase variable. Each RBF, given by Equation (5), is characterized by a center $c_i$ and a bandwidth parameter $h_i$.
For a given $N$ and setting $\tau$ equal to the total duration of the desired movement, we have $c_i = \exp\left(-\frac{\alpha_x(i-1)}{N-1}\right)$ and $h = \frac{1}{N(N-1)}\sum_{i=1}^{N-1}(c_{i+1}-c_i)^2$. Each degree of freedom (DoF) requires adjusting the weights $w_i$ based on the measured data to achieve the desired behavior. The number of weights should be chosen based on the desired resolution of the trajectory. When controlling a robotic system with multiple DoFs, we represent the movement of each DoF using its own equation system (1)-(2), but with a common phase (3) to synchronize them.

For a discrete motion, given a demonstrated trajectory $y_d(t_k)$, where $t_k = 1, \ldots, T$, and its time derivatives $\dot{y}_d(t_k)$ and $\ddot{y}_d(t_k)$, it is possible to invert Equation (1) and approximate the desired shape $f_d$ as follows:

\begin{equation}
f_d(t_k) = \frac{\tau^2 \ddot{y}_d(t_k) - \alpha_z \left(\beta_z (g - y_d(t_k)) - \tau \dot{y}_d(t_k)\right)}{\alpha_z}.
\label{eq:forcing-term}
\end{equation}

Equation (\ref{eq:forcing-term}) provides an approximation of the desired shape $f_d(t_k)$ based on the demonstrated trajectory and its time derivatives.

By stacking each $f_d(t_k)$ and $w_i$ into the column vectors $F = \begin{pmatrix} f_d(t_1) \\ \vdots \\ f_d(t_T) \end{pmatrix}$ and $w = \begin{pmatrix} w_1 \\ \vdots \\ w_N \end{pmatrix}$, we obtain the following linear system:

\begin{equation}
\Phi w = F, 
\end{equation}

where
\begin{equation}
\Phi = \begin{bmatrix} 
    \frac{\Psi_1(x_1)}{\sum_{i=1}^{N} \Psi_i(x_1)x_1} & \ldots & \frac{\Psi_N(x_1)}{\sum_{i=1}^{N} \Psi_i(x_1)x_1} \\
    \vdots & \ddots & \vdots\\
    \frac{\Psi_1(x_T)}{\sum_{i=1}^{N} \Psi_i(x_1)x_1} & \ldots & \frac{\Psi_N(x_T)}{\sum_{i=1}^{N} \Psi_i(x_1)x_1} \\
    \end{bmatrix}
\end{equation} \newline

The phase variable $x$ in Equation (3) enables the user to slow or accelerate the motion: this is the phase-stopping mechanism \cite{ijspeert_movement_2002}:

\begin{equation}
\frac{\tau_x \dot{x}}{\alpha_x} = -1 + \alpha_y x \left| \tilde{y} - y \right|, 
\end{equation}

Additionally, one useful feature of the DMP is the possibility to adapt the trajectory online if the goal changes \cite{ijspeert_dynamical_2013}:

\begin{equation}
\tau_g \dot{g} = \alpha_g (g_0 - g). 
\end{equation}

However, the standard formulation of DMPs is not suitable for directly encoding skills with specific geometric constraints, such as orientation profiles represented by unit quaternions. For instance, direct integration of unit quaternions does not ensure the unity of the quaternion's norm. Thus, the equations of the classical discrete DMP have to be adapted in order to be generalizable to quaternions. The equations system can be found in this tutorial survey about DMP \cite{saveriano_dynamic_2021}. Even though the core ideas are the same, one difference is the introduction of the \emph{quaternion logarithm} \(\text{Log}^q (\boldsymbol{gq}^* \boldsymbol{\bar{q}})\) that has an impact when it comes to computation. This expression defines the angular velocity \(\omega\) that rotates quaternion \(q\) into \(gq\) within a unit sampling time.

The function \(\text{Log}_q(\cdot) : S^3 \rightarrow \mathbb{R}^3\) is defined as:

\[
\text{Log}_q(q) = \begin{cases}
\arccos(\nu) \frac{q}{\|q\|}, & \text{if } q \neq 0 \\
[0 \ 0 \ 0]^T, & \text{otherwise}
\end{cases}
\]

This definition maps a quaternion \(q\) from the unit sphere \(S^3\) to a vector in \(\mathbb{R}^3\) using the arccosine function and normalization. 

\section{Learning from Demonstrations} \label{LfD}

Scheme of all the framework

\subsection{Gathering demonstrations}


Building a dataset that covers a wide range of configurations is essential to reach an efficient generalization of the task, hence providing mutliple demonstrations to the robot. To do so, we demonstrated the same task in a similar environment (in the same room, with similar objects on the table, ...) but with different conditions. These include different position parameters (poses of the object, goal poses, grasping poses) as well as variate dynamic variables (speed of the movement). Having such set of demonstrations limits the \textit{covariate  shift}, that's to say the difference of distribution between the training dataset and the real-world cases. Nonetheless, we can not get rid of  this covariate shift in the extent that the action space of a given task in the real world is most often far bigger than what can represent a few demonstrations. In addition to that, our approach is to have a dataset corresponding to what might come natural to a non-professional user.

Once the demonstrations gathered, the choice of the features to study is another core step that often requires the experience of a skilled user. Whereas a great majority of LfD techniques selects manually the relevant features to study, we chose to take the measurements of all the sensors available as well as the proprioceptive information, namely 6D Cartesian position and orientation and angular and linear velocites obtained from the robot's solver. Mainly two reasons justify that:

 \begin{itemize}
     \item A non-skilled user has to be able to demonstrate a new task only by achieving it in a  natural way while wearing the teleoperation device
     \item Capitalize on our similarity with humanoid robots. Indeed, integrating sensory parameters amounts to add a kind of \textit{sensory integration} to the robot. \textit{Sensory integration} was first theorized by Dr A. Jeans Ayres in 1972 in the field of neuroscience\fk{reference}. It states that when thinking about achieving some known task, our brain processes not only how to reach the goal (the different steps), but also the sensations we had when priorly achieving this task. This idea was first exploited by P. Pastor in \cite{sensory_skill}. A telling example of sensory memory is that when we grab an object, the brain area corresponding to our sense of touch activates, and we expect to have some sensation when grabbing an object. In \cite{sensory_skill}, a robot arm corrects his trajectory during grabbing by adding a term that corresponds to his expected force feedback on the gripper. Thus, taking sensor into account while building the dataset can help to detect new phases of the movement \cite{sensory_seg} and correcting the movement online (after the learning phase). \newline

\end{itemize}

 We first design a controller to build a dataset in  smulation on the JVRC1 virtual humanoid. To that end, we designed a finite state machine (FSM) controller with B-splines with mc-rtc and mc-mujoco \cite{singh2023mc} to simulate a one-handed grasping task of a stick in 9 different configurations (figure with mujoco interface and 2D schema). Then, we build a real-world dataset, with the JRL's teleoperation device  with the Kaleido robot (figure  of experimental setup). The user has a the visual of the humanoid's front camera, and achieves the task naturally, except that there is no force  feedback on the grippers, meaning that the demonstrator is not constrained by the physical shape of the object during the demonstrations.

 In both cases, mc-rtc logs all the sensor and proprioceptive values that are then used to learn and generalize the task. The task can be represented by all its normalized feature values in skill maps (figure skill map). Using this map, we have to extract appropriate features and segment it into interpretable phases.

\subsection{Feature Selection}

Having gathered a set of demonstrations containing numerous features, we now want to automatically extract the most relevant features to segment the task in skills reproducible with DMP.

A first step is to temporally align the signals in order to compare the features across the dataset. Indeed, our goal is to find the most correlated features. Intuitively, if one feature has the same shape in all the trials, it is likely that it will be useful for segmentation, and that it is critical to define the task. For example, in a grasping task, the opening of the gripper clearly identifies the different phases of the movement and has the same shape across all the trials. On the contrary, we expect that other parameters like the position along the z-axis have no importance in the grasping task.

To align the signals, we use Dynamical Time Warping (DTW). The main idea behind DTW is to find the optimal alignment between the two sequences by warping their time axes. This warping process allows for the matching of similar patterns, despite variations in timing and duration. It works by finding the minimum distance path through a grid or matrix that represents the pairwise distances between elements of the two sequences. \newline


We can then compute a \textit{correlation matrix} for each feature across all trials (figure of a correlation matrix). The mean of the coefficients of these matrices can therefore be interpreted as the similarity across the datasets. This first result, albeit convincing, include a bias due to the temporal alignment of the signals. Indeed, it is possible to end up with a highly correlated feature when the signals are aligned, but in reality very distant. In fact, the distance computed with DTW can also be used as a tool to measure similarity. Taking again the statistical mean of the two by two distances for every feature and normalizing the values between 0 and 1, we can penalize the correlation coefficients computed afterwards. We employ the normalized DTW distances as weights for the correlation coefficients. This allows to take into account the distance between the trials, eventually leading to a more objective measure of the relevance of the features. We see in (figure hist weighted) that the most correlated features in (hist normal) are even more detached from all the other features.  

Finally, we select the features to pass in argument for the segmentation phase. There is a trade-off between selecting the most correlated features and discarding others - thus possibly missing important information - and choosing too much, - thus thwarting the segmentation with uncorrelated features -. In  addition to the most correlated feature $F$, we select the features that are above $CorrCoef_{F} \times T$, with $T$ a threshold that we set at $0.9$. \newline

% Signature of the task for further recognition

\textbf{Note:} This correlation analysis is only relevant for demonstrations that are long enough (more than a few seconds). Otherwise, the p-value of the Pearson coefficient \cite{pearson} is above 0.05 and the correlation is very likely to be irrelevant.

\subsection{Segmentation}

% Display the different results here in this part and just comparing the segmentation with and without the preprocessing/cleaning in results or just say what we do (parameters, algorithms) and display the different results of the segmentation in results, thus adding another subpart
Parameters used, which library
If we clean the data or not
Once we have possible breakpoints, we filter them with sensory information (cite detection with sensory information for insertion task

% Adding a paragraph with corroboration of sensory information with Hamming window ?

\subsection{Task reproduction}

Explain how to recognize a task (shapelets, signature), prendre les points de demos les plus proches et extrapoler les poids

\section{Results} \label{results}

\subsection{Segmentation on multivariate raw data}

TODO

\subsection{Segmentation on multivariate preprocessed data}

TODO 

\subsection{Task recognition and reproduction}

How to recognize a task that has already been shown

\section{Discussion}\label{discussion}

\subsection{Comparative study and limits}

TODO take results of other articles
6d pose estimation
statistical approach to select relevant features
library of skills

\subsection{Conclusion and future work}

Throughout this work, we presented an end-to-end framework to learn and generalize a new task for humanoid robot by automatically segment demonstrations provided in a natural way through teleoperation into reproducible skills. In addition as being a novelty for complex humanoid robots, the proposed approach can be extented to every humanoid robot, and theoretically to every combination of end-effectors (one hand, two hands, legs). On top of that, automatically extracting the relevant features to learn the task with a correlation analysis corroborated with sensory information enables non-skilled users to learn tasks to the robots, therefore fostering humanoids integration in real-world work sites. [Whether validated by the simulation and real trial.] \newline


This work mainly paves the for two potential prolongations.
\begin{itemize}
    \item \textit{Increase the efficiency and the automation.} This would include compute 6d-pose estimation of objects based on visual input, or refine our DMP implementation to make it more efficient, add goal-switching and obstacle avoidance as stated in \cite{saveriano_dynamic_2021}. 
    
    \item  \textit{Increase the generalizability and the objectivity.} One could think about replacing the arbitrary thresholds in the feature selection by statistical methods, or trying to avoid covariate shift by generating demonstrations through meta-learning \cite{yu_one-shot_2018}.    

\end{itemize}
\bibliography{references.bib}
\bibliographystyle{IEEEtran}


\vspace{12pt}

\end{document}
